{% extends "base.html" %}

{% block title %}Settings - MedMatchINT{% endblock %}

{% block content %}
<div class="header">
  <h1 class="header-title">Settings</h1>
  <p class="header-subtitle">Configure LLM model parameters</p>
</div>

<div class="card">
  <div class="card-body">
    <form id="settings-form">
      <div class="form-group">
        <label for="model-select">Select Model</label>
        <!-- Curtain menu (dropdown) populated dynamically from /api/models -->
        <select id="model-select" class="form-control">
          <!-- Options loaded by loadModels() -->
        </select>
      </div>

      <div class="form-group">
        <label for="context-size">Context Size (tokens)</label>
        <input type="number" id="context-size" class="form-control" min="512" max="131072" step="512" />
      </div>

      <div class="form-group">
        <label for="temperature">Temperature</label>
        <input type="number" id="temperature" class="form-control" min="0" max="1" step="0.01" />
      </div>

      <div class="form-group">
        <label for="batch-size">Trial Matching Batch Size</label>
        <input type="number" id="batch-size" class="form-control" min="1" max="100" step="1" />
      </div>

      <button type="submit" class="btn btn-primary">Save Settings</button>
    </form>
  </div>
</div>

<script>
  // Populate the dropdown from /api/models, then load persisted settings
  async function loadSettings() {
    await loadModels();
    const res = await fetch('/api/settings');
    const data = await res.json();

    // Apply persisted config (with defaults for safety)
    document.getElementById('model-select').value = data.LLM_MODEL || "llama3.1:8b-custom";
    document.getElementById('context-size').value = data.LLM_CONTEXT_SIZE ?? 8192;
    document.getElementById('temperature').value = data.LLM_TEMPERATURE ?? 0.1;
    document.getElementById('batch-size').value = data.TRIAL_MATCHING_BATCH_SIZE ?? 4;
  }

  document.getElementById('settings-form').addEventListener('submit', async (e) => {
    e.preventDefault();
    const newSettings = {
      LLM_MODEL: document.getElementById('model-select').value,
      LLM_CONTEXT_SIZE: parseInt(document.getElementById('context-size').value, 10),
      LLM_TEMPERATURE: parseFloat(document.getElementById('temperature').value),
      TRIAL_MATCHING_BATCH_SIZE: parseInt(document.getElementById('batch-size').value, 10)
    };

    const res = await fetch('/api/settings', {
      method: 'POST',
      headers: {'Content-Type': 'application/json'},
      body: JSON.stringify(newSettings)
    });

    const result = await res.json();
    alert(result.message || "Settings saved!");
  });

  // Load available models from the backend
  async function loadModels() {
    const select = document.getElementById('model-select');
    select.innerHTML = '';

    try {
      const res = await fetch('/api/models');
      const payload = await res.json();

      // Accept both {"models": [...]} and [...] for backward compatibility
      const models = Array.isArray(payload) ? payload : (payload.models || []);
      if (models.length === 0) throw new Error('No models returned');

      // Preserve current selection if present
      const currentValue = select.value;

      models.forEach(model => {
        const opt = document.createElement('option');
        opt.value = model;
        opt.text = model;
        select.appendChild(opt);
      });

      // Restore previous selection if it still exists
      if (currentValue && models.includes(currentValue)) {
        select.value = currentValue;
      }
    } catch (e) {
      // Fallback to a safe default (matches your current config default)
      const fallback = ["llama3.1:8b-custom"];
      fallback.forEach(model => {
        const opt = document.createElement('option');
        opt.value = model;
        opt.text = model;
        select.appendChild(opt);
      });
    }
  }

  window.onload = loadSettings;
</script>
{% endblock %}
